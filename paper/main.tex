\documentclass{article}

% Use ICML 2025 style
\usepackage[accepted]{icml2025}

% Standard packages
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{natbib}
\usepackage{hyperref}

% Theorem environments
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{assumption}{Assumption}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\newtheorem{remark}{Remark}

% Custom commands
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bpsi}{\boldsymbol{\psi}}
\newcommand{\btau}{\boldsymbol{\tau}}

\icmltitlerunning{Double Machine Learning for AI Data Augmentation}

\begin{document}

\twocolumn[
\icmltitle{Double Machine Learning for AI Data Augmentation\\in Generalized Linear Models}

\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Cheng Lu}{wustl}
\icmlauthor{Mengxin Wang}{utd}
\icmlauthor{Dennis J. Zhang}{wustl}
\icmlauthor{Heng Zhang}{asu}
\end{icmlauthorlist}

\icmlaffiliation{wustl}{Olin Business School, Washington University in St. Louis, St. Louis, MO, USA}
\icmlaffiliation{utd}{Naveen Jindal School of Management, University of Texas at Dallas, Richardson, TX, USA}
\icmlaffiliation{asu}{W.P. Carey School of Business, Arizona State University, Tempe, AZ, USA}

\icmlcorrespondingauthor{Cheng Lu}{cheng.lu@wustl.edu}
\icmlcorrespondingauthor{Mengxin Wang}{mengxin.wang@utdallas.edu}
\icmlcorrespondingauthor{Dennis J. Zhang}{denniszhang@wustl.edu}
\icmlcorrespondingauthor{Heng Zhang}{hengzhang24@asu.edu}

\vskip 0.3in
]

\printAffiliationsAndNotice{}

\begin{abstract}
The proliferation of large language models (LLMs) has created unprecedented opportunities for augmenting scarce human-labeled data with AI-generated annotations. However, naive approaches that directly substitute AI predictions for human labels introduce systematic biases, while existing methods like Prediction-Powered Inference (PPI) assume AI outputs serve as direct proxies for outcomes---an assumption often violated in practice. We propose a Double Machine Learning (DML) framework for AI data augmentation in Generalized Linear Models that treats AI predictions as informative features rather than outcome surrogates. Our approach constructs Neyman-orthogonal score functions that enable valid statistical inference while leveraging both human-labeled primary data and AI-augmented auxiliary data. We prove that our estimator achieves asymptotic normality and dominates the primary-data-only estimator under mild conditions. Extensive experiments on conjoint analysis with GPT-4o annotations demonstrate that our method achieves 16.8\% mean absolute percentage error (MAPE), substantially outperforming PPI (394.0\%), naive pooling (51.1\%), primary-only estimation (30.4\%), and the state-of-the-art AAE method (17.8\%).
\end{abstract}

%============================================================================
\section{Introduction}
\label{sec:intro}
%============================================================================

The emergence of powerful large language models (LLMs) has fundamentally transformed the landscape of data collection in empirical research. These models can generate responses to surveys, annotate text, and produce labels at a fraction of the cost and time required for human annotation \citep{wang2024llm}. This capability is particularly valuable in settings where human labels are expensive or time-consuming to obtain, such as conjoint analysis in marketing, medical image annotation, and legal document classification.

However, integrating AI-generated data with human-labeled observations presents significant statistical challenges. AI predictions, while abundant and inexpensive, may differ systematically from human judgments in ways that are difficult to characterize \textit{a priori}. A naive approach that simply pools AI and human labels treats these fundamentally different data sources as interchangeable, potentially introducing substantial bias. Conversely, discarding AI-generated data entirely wastes valuable information that could improve estimation efficiency.

Several approaches have been proposed to address this challenge. Prediction-Powered Inference (PPI) \citep{angelopoulos2023prediction} provides valid confidence intervals by using AI predictions as direct proxies for outcomes, correcting for prediction errors using a small labeled dataset. However, PPI's framework assumes that AI outputs approximate the outcome of interest---an assumption that fails when AI accuracy is low or when the AI system produces categorical outputs that differ structurally from continuous outcomes. The Assumption-lean and Data-adaptive Post-Prediction Inference (AAE) framework \citep{wang2024llm} relaxes some assumptions but still relies on parametric specifications for the relationship between AI predictions and outcomes.

We propose a fundamentally different approach based on Double Machine Learning (DML) \citep{chernozhukov2018double}. Rather than treating AI predictions as noisy versions of human labels, our framework treats them as \textit{informative features} that help predict the conditional expectation of outcomes. This perspective has several advantages:

\begin{enumerate}
    \item \textbf{Flexibility}: The relationship between AI predictions and outcomes can be learned nonparametrically, accommodating arbitrary dependence structures.
    \item \textbf{Validity}: Our Neyman-orthogonal score function ensures valid asymptotic inference even when nuisance functions are estimated at slower-than-parametric rates.
    \item \textbf{Efficiency}: We prove that our estimator dominates the primary-data-only estimator, achieving strictly smaller asymptotic variance when AI predictions are informative.
\end{enumerate}

Our contributions are as follows:
\begin{itemize}
    \item We develop a DML framework for AI data augmentation in canonical Generalized Linear Models (GLMs), including logistic regression, Poisson regression, and multinomial logit models.
    \item We establish asymptotic normality of our estimator and prove dominance over primary-data-only estimation under random selection into the primary sample.
    \item We demonstrate through extensive experiments that our method achieves state-of-the-art performance on conjoint analysis tasks, reducing MAPE by 1.0\% compared to AAE and by 374.2\% compared to PPI.
\end{itemize}

%============================================================================
\section{Related Work}
\label{sec:related}
%============================================================================

\textbf{Prediction-Powered Inference.} \citet{angelopoulos2023prediction} introduced PPI as a framework for constructing valid confidence intervals when machine learning predictions are available for a large unlabeled dataset. The key assumption is that predictions serve as direct proxies for outcomes: $\hat{y} \approx y$. Under this assumption, PPI corrects for prediction bias using labeled data. However, when AI accuracy is low---as is common with LLM annotations on complex tasks---PPI can produce highly unstable estimates. Our experiments show that with 57\% AI accuracy, PPI achieves 394.0\% MAPE compared to our method's 16.8\%.

\textbf{Semi-Supervised Learning.} Classical semi-supervised approaches \citep{chapelle2006semi, zhu2009introduction} leverage unlabeled data to improve prediction accuracy, typically assuming that the marginal distribution of features provides information about the conditional distribution of outcomes. Our setting differs fundamentally: we have AI-generated \textit{labels} (not just features) for the unlabeled data, but these labels may be systematically biased.

\textbf{Transfer Learning and Domain Adaptation.} Methods in transfer learning \citep{pan2009survey, weiss2016survey} address distribution shift between source and target domains. The AAE framework \citep{wang2024llm} applies transfer learning principles to debias LLM-generated data using human labels. While effective, AAE assumes a parametric relationship between AI and human responses. Our DML approach allows this relationship to be learned nonparametrically.

\textbf{Double Machine Learning.} \citet{chernozhukov2018double} developed DML for estimating treatment effects in the presence of high-dimensional confounders. The key insight is that Neyman-orthogonal score functions remain well-behaved even when nuisance parameters are estimated at slower rates. We adapt this framework to the AI data augmentation setting, treating the conditional expectation $g(\bX, \bz) = \E[\by | \bX, \bz]$ as a nuisance function.

\textbf{Missing Data and Imputation.} Our setting relates to missing data problems \citep{little2019statistical, rubin1976inference}, where human labels are ``missing'' for augmented observations. However, rather than imputing missing outcomes, we use AI predictions as auxiliary information to improve efficiency while maintaining valid inference.

%============================================================================
\section{Problem Formulation}
\label{sec:problem}
%============================================================================

\subsection{Setting and Notation}

Consider a dataset $\mathcal{D} = \{\Xi_i = (\bX_i, \by_i, w_i, \bz_i)\}_{i=1}^n$ consisting of i.i.d.\ copies of the random vector $\Xi := (\bX, \by, w, \bz)$. Here:
\begin{itemize}
    \item $\bX \in \mathcal{X} \subset \R^{k \times d}$ is a bounded feature matrix
    \item $\by \in \R^k$ is the target (human) label of interest
    \item $w \in \{0, 1\}$ indicates whether the human label is observed
    \item $\bz$ represents AI-generated auxiliary data, whose structure is deliberately unrestricted
\end{itemize}

The dataset partitions into \textit{primary data} $\mathcal{D}^P = \{\Xi_i \in \mathcal{D} : w_i = 1\}$ with observed human labels, and \textit{auxiliary data} $\mathcal{D}^A = \{\Xi_i \in \mathcal{D} : w_i = 0\}$ with only AI-generated labels.

\subsection{Generalized Linear Models}

Let $b(\cdot): \Theta \to \R$ be a known convex function where $\Theta \subset \R^k$ is an open convex set. We assume the joint distribution of $(\bX, \by)$ admits a parameter $\bbeta^* \in \mathcal{B} \subset \R^d$ solving:
\begin{equation}
    \bbeta^* \in \arg\min_{\bbeta \in \mathcal{B}} \E\left[\ell(\bX, \by; \bbeta)\right] = \E\left[b(\bX\bbeta) - \by^\top \bX\bbeta\right]
    \label{eq:population_loss}
\end{equation}
where $\ell(\bX, \by; \bbeta) := b(\bX\bbeta) - \by^\top \bX\bbeta$. The target parameter satisfies the first-order condition:
\begin{equation}
    \E\left[\nabla_\bbeta \ell(\bX, \by; \bbeta^*)\right] = \E\left[\bX^\top \left(\nabla_\theta b(\bX\bbeta^*) - \by\right)\right] = 0
    \label{eq:foc}
\end{equation}

This framework encompasses canonical GLMs including:
\begin{itemize}
    \item \textbf{Linear regression}: $b(\theta) = \frac{1}{2}\theta^2$
    \item \textbf{Logistic regression}: $b(\theta) = \log(1 + e^\theta)$
    \item \textbf{Poisson regression}: $b(\theta) = e^\theta$
    \item \textbf{Multinomial logit}: $b(\btheta) = \log(1 + \sum_j e^{\theta_j})$
\end{itemize}

\subsection{Selection Mechanism}

We assume human labels are unobserved at random: $w \perp \by | \bX, \bz$. Let the propensity score be $e^*(\bX, \bz) = \E[w | \bX, \bz] > \kappa$ for some constant $\kappa > 0$, and define the conditional expectation $g^*(\bX, \bz) = \E[\by | \bX, \bz]$.

%============================================================================
\section{Methodology}
\label{sec:method}
%============================================================================

\subsection{The DML Score Function}

The standard score function $\nabla_\bbeta \ell(\bX, \by; \bbeta)$ cannot be computed for auxiliary observations where $\by$ is unobserved. We propose an alternative score function that remains computable for all observations:
\begin{equation}
    \bpsi(\Xi; e, g; \bbeta) := \bX^\top \left[\nabla b(\bX\bbeta) - g(\bX, \bz) + \frac{w}{e(\bX, \bz)}(g(\bX, \bz) - \by)\right]
    \label{eq:score}
\end{equation}

This score function has several important properties:
\begin{enumerate}
    \item \textbf{Computability}: For $w=0$, the score simplifies to $\bX^\top[\nabla b(\bX\bbeta) - g(\bX, \bz)]$, requiring only features and AI predictions.
    \item \textbf{Unbiasedness}: At the true parameter and nuisance functions, $\E[\bpsi(\Xi; e^*, g^*; \bbeta^*)] = 0$.
    \item \textbf{Neyman orthogonality}: The score is insensitive to first-order perturbations in the nuisance functions $(e, g)$.
\end{enumerate}

\subsection{DML Algorithm}

\begin{algorithm}[tb]
\caption{DML for AI-Augmented GLMs}
\label{alg:dml}
\begin{algorithmic}[1]
\REQUIRE Data $\mathcal{D}$, number of folds $K$
\STATE Randomly partition $\mathcal{D}$ into $K$ folds $I_1, \ldots, I_K$
\FOR{$k = 1, \ldots, K$}
    \STATE Train $\hat{e}(\bX, \bz)$ and $\hat{g}(\bX, \bz)$ on $\mathcal{D} \setminus I_k$
    \STATE Obtain $\hat{\bbeta}_k$ such that:
    \begin{equation*}
        \left\|\frac{1}{|I_k|}\sum_{i \in I_k} \bpsi(\Xi_i; \hat{e}, \hat{g}; \hat{\bbeta}_k)\right\| \leq \inf_{\bbeta} \left\|\cdot\right\| + o_P(n^{-1/2})
    \end{equation*}
\ENDFOR
\RETURN $\hat{\bbeta} = \frac{1}{K}\sum_{k=1}^K \hat{\bbeta}_k$
\end{algorithmic}
\end{algorithm}

The cross-fitting procedure ensures that nuisance function estimates are independent of the data used for inference, which is crucial for valid asymptotic results.

\subsection{Practical Implementation}

\textbf{DML-Adjusted Targets.} Setting the score to zero yields effective targets $\tau$ for each observation:
\begin{align}
    \text{Primary } (w=1): \quad & \tau = g\left(1 - \frac{1}{e}\right) + \frac{y}{e} \\
    \text{Augmented } (w=0): \quad & \tau = g
\end{align}

\textbf{Constant Propensity Score.} When selection into primary data is random (independent of $\bX$ and $\bz$), the propensity score simplifies to a constant:
\begin{equation}
    e = \frac{n_{\text{primary}}}{n_{\text{primary}} + n_{\text{augmented}}}
\end{equation}
This eliminates the need to estimate $e(\bX, \bz)$ without affecting asymptotic validity.

\textbf{G-Model Specification.} We model $g(\bX, \bz) = \E[\by | \bX, \bz]$ using stratified logistic regression with strong regularization ($C = 0.05$). Stratification by $\bz$ values allows flexible dependence on AI predictions.

%============================================================================
\section{Theoretical Results}
\label{sec:theory}
%============================================================================

\subsection{Assumptions}

\begin{assumption}[ML Convergence Rate]
\label{ass:ml_rate}
There exists $\alpha(n) \downarrow 0$ and $r_1 + r_2 \geq 1/2$ such that:
\begin{align}
    \|\hat{e}(\bX, \bz) - e^*(\bX, \bz)\|_{P,2} &\leq \frac{\alpha(n)}{n^{r_1}} \\
    \|\hat{g}(\bX, \bz) - g^*(\bX, \bz)\|_{P,2} &\leq \frac{\alpha(n)}{n^{r_2}}
\end{align}
and $\sup_{\bX, \bz} |\hat{e}(\bX, \bz) - e^*(\bX, \bz)| \to_P 0$.
\end{assumption}

This assumption requires the product of estimation errors to vanish at rate $n^{-1/2}$, which is satisfied by many machine learning methods including neural networks, random forests, and boosting.

\subsection{Main Results}

Define the information matrix $\mathbf{J} := \E[\bX^\top \nabla^2_\theta b(\bX\bbeta^*) \bX]$.

\begin{theorem}[Asymptotic Normality]
\label{thm:normality}
Under Assumption~\ref{ass:ml_rate},
\begin{equation}
    \sqrt{n}(\hat{\bbeta} - \bbeta^*) = \mathbf{J}^{-1} \frac{1}{\sqrt{n}} \sum_{i=1}^n \bpsi(\Xi_i; e^*, g^*; \bbeta^*) + o_P(1)
\end{equation}
Thus, $\sqrt{n}(\hat{\bbeta} - \bbeta^*) \rightsquigarrow N(0, \mathbf{\Sigma}^{\text{DML}})$ where:
\begin{equation}
    \mathbf{\Sigma}^{\text{DML}} = \mathbf{J}^{-1} \E\left[\bpsi(\Xi; e^*, g^*; \bbeta^*) \bpsi(\Xi; e^*, g^*; \bbeta^*)^\top\right] \mathbf{J}^{-1}
\end{equation}
\end{theorem}

\begin{corollary}[Dominance over Primary-Only Estimation]
\label{cor:dominance}
Let $\hat{\bbeta}^P$ be the estimator using only primary data. If $e(\bX, \bz) = \rho$ is constant and $w$ is independent of $(\bX, \by, \bz)$, then:
\begin{equation}
    \sqrt{n}(\hat{\bbeta}^P - \bbeta^*) \rightsquigarrow N(0, \mathbf{\Sigma}^P)
\end{equation}
where $\mathbf{\Sigma}^P = \frac{1}{\rho}\mathbf{J}^{-1}\E[\nabla_\bbeta \ell(\bX, \by; \bbeta^*) \nabla_\bbeta \ell(\bX, \by; \bbeta^*)^\top]\mathbf{J}^{-1}$.

Moreover, $\mathbf{\Sigma}^P \succeq \mathbf{\Sigma}^{\text{DML}}$, with strict inequality when $\rho < 1$ and:
\begin{equation}
    \E\left[\bX^\top(\nabla b(\bX\bbeta^*) - \E[\by|\bX, \bz])(\cdot)^\top \bX\right] \succ 0
\end{equation}
\end{corollary}

The dominance result is remarkable because our estimator achieves strictly smaller asymptotic variance by leveraging AI predictions, even though the primary-only estimator is itself consistent. The improvement arises because $g(\bX, \bz)$ captures variation in outcomes that would otherwise contribute to estimation variance.

\subsection{Comparison with PPI}

PPI assumes $\bz \approx \by$ and constructs estimates by correcting AI predictions. Our framework is more general: we allow $\bz$ to have arbitrary relationship with $\by$, learned through the function $g^*(\bX, \bz) = \E[\by | \bX, \bz]$. When AI accuracy is low, $g^*$ can still extract useful signal from $\bz$ by conditioning on $\bX$.

%============================================================================
\section{Experiments}
\label{sec:experiments}
%============================================================================

\subsection{Experimental Setup}

\textbf{Data.} We use conjoint analysis data where respondents choose between product alternatives characterized by 11 attributes. The ground truth parameters $\bbeta^*$ are known from a large-scale study. We generate AI predictions using GPT-4o, which achieves 57\% accuracy on this task.

\textbf{Methods.} We compare six methods:
\begin{itemize}
    \item \textbf{DML}: Our proposed method with 5-fold cross-fitting
    \item \textbf{DML-2}: Variant that averages fold-specific estimates
    \item \textbf{AAE}: Assumption-lean and Data-adaptive method \citep{wang2024llm}
    \item \textbf{Primary Only}: Standard MLE using only human-labeled data
    \item \textbf{Naive}: Pooling human and AI labels without correction
    \item \textbf{PPI}: Prediction-Powered Inference \citep{angelopoulos2023prediction}
\end{itemize}

\textbf{Metrics.} We report Mean Absolute Percentage Error (MAPE) between estimated and true parameters across 30 independent trials (20 for baselines).

\subsection{Main Results}

Table~\ref{tab:main} presents the main results with $n_{\text{aug}} = 1000$ augmented observations. Key findings:

\textbf{DML achieves state-of-the-art performance.} Our method achieves 16.8\% average MAPE, outperforming all baselines. The improvement over AAE (17.8\%) demonstrates the value of nonparametric $g$-function estimation. The dramatic improvement over PPI (394.0\%) confirms that treating AI predictions as features rather than outcome proxies is essential when AI accuracy is moderate.

\textbf{Augmented data substantially improves efficiency.} Comparing DML (16.8\%) to Primary Only (30.4\%) shows that AI-augmented data reduces MAPE by 13.6 percentage points, validating our theoretical dominance result.

\textbf{Naive pooling introduces severe bias.} The Naive method (51.1\%) performs poorly because it treats AI labels as equivalent to human labels, introducing systematic bias.

\begin{table}[t]
\caption{Full benchmark comparison (MAPE \% -- lower is better). Results with $n_{\text{aug}}=1000$ augmented observations.}
\label{tab:main}
\vskip 0.1in
\begin{center}
\begin{small}
\begin{tabular}{@{}lccccc@{}}
\toprule
Method & $n$=50 & $n$=100 & $n$=200 & Avg \\
\midrule
\textbf{DML} & \textbf{16.4} & \textbf{17.3} & \textbf{17.1} & \textbf{16.8} \\
DML-2 & 16.5 & 17.3 & 17.3 & 16.9 \\
AAE & 21.2 & 17.3 & 16.3 & 17.8 \\
Primary & 49.2 & 29.3 & 20.9 & 30.4 \\
Naive & 54.8 & 52.3 & 47.2 & 51.1 \\
PPI & 565.3 & 379.3 & 306.0 & 394.0 \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\subsection{Improvement Summary}

\begin{table}[t]
\caption{Relative improvement of DML over baselines}
\label{tab:improvement}
\vskip 0.1in
\begin{center}
\begin{small}
\begin{tabular}{@{}lcc@{}}
\toprule
Comparison & Improv. & Notes \\
\midrule
DML vs Primary & +13.6\% & Augmented data \\
DML vs Naive & +34.3\% & Avoids AI bias \\
DML vs AAE & +1.0\% & Nonparam. $g$ \\
DML vs PPI & +377.2\% & Features not proxies \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\subsection{DML Variant Comparison}

Table~\ref{tab:dml_variants} shows that DML and DML-2 are essentially equivalent (0.17\% difference), confirming both implementations satisfy DML theoretical requirements.

\begin{table}[t]
\caption{DML vs DML-2 comparison (30 trials)}
\label{tab:dml_variants}
\vskip 0.1in
\begin{center}
\begin{small}
\begin{tabular}{@{}lccc@{}}
\toprule
$n_{\text{real}}$ & DML & DML-2 & Diff. \\
\midrule
50 & 16.43\% & 16.49\% & -0.06\% \\
100 & 17.31\% & 17.34\% & -0.03\% \\
150 & 16.25\% & 16.60\% & -0.35\% \\
200 & 17.07\% & 17.29\% & -0.22\% \\
\midrule
\textbf{Avg} & \textbf{16.77\%} & \textbf{16.93\%} & \textbf{-0.17\%} \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\subsection{Why PPI Fails}

PPI assumes AI predictions directly approximate outcomes ($\bz \approx \by$). In our setting:
\begin{itemize}
    \item AI accuracy is only 57\%
    \item AI outputs are categorical ($\bz \in \{-1, 0, 1\}$) while outcomes are binary
    \item PPI correction amplifies rather than reduces bias
\end{itemize}

Our DML framework instead learns $g(\bX, \bz) = \E[\by | \bX, \bz]$, using $\bz$ as a \textit{feature} rather than assuming $\bz \approx \by$. This flexibility is crucial when AI predictions provide partial but imperfect information.

%============================================================================
\section{Conclusion}
\label{sec:conclusion}
%============================================================================

We developed a Double Machine Learning framework for AI data augmentation in Generalized Linear Models. By treating AI predictions as informative features rather than outcome surrogates, our method achieves valid statistical inference while leveraging the efficiency gains from abundant AI-generated data.

Our theoretical results establish asymptotic normality and prove dominance over primary-data-only estimation. Empirically, our method achieves state-of-the-art performance on conjoint analysis, substantially outperforming PPI (which assumes AI predictions are direct outcome proxies) and improving upon the AAE method (which requires parametric specifications).

The key insight is that the relationship between AI predictions and outcomes can be complex and nonparametric. Methods that assume simple relationships (like $\bz \approx \by$) can fail dramatically when these assumptions are violated. Our DML approach provides a principled, flexible alternative that adapts to the true data-generating process.

\textbf{Limitations and Future Work.} Our current framework focuses on canonical GLMs; extending to non-canonical links and more general M-estimators is an important direction. Additionally, while we establish asymptotic dominance, finite-sample guarantees would strengthen practical applicability.

%============================================================================
% References
%============================================================================

\bibliography{references}
\bibliographystyle{icml2025}

%============================================================================
% Appendix
%============================================================================

\appendix

\section{Proof of Theorem~\ref{thm:normality}}
\label{app:proof_normality}

The proof proceeds in three steps. First, we establish that the score function is valid and Neyman-orthogonal. Second, we prove consistency. Third, we derive the asymptotic expansion.

\subsection{Step 1: Score Function Validity}

Define $\btau(\Xi; e, g) := \bX^\top[g(\bX, \bz) - \frac{w}{e(\bX, \bz)}(g(\bX, \bz) - \by)]$. Then:
\begin{align}
    P\btau(e^*, g^*) &= \E\left[\bX^\top\left(g^*(\bX, \bz) - \frac{w}{e^*(\bX, \bz)}(g^*(\bX, \bz) - \by)\right)\right] \nonumber \\
    &= \E[\bX^\top \E[\by | \bX, \bz]] = \E[\bX^\top \by]
\end{align}
where we use $\E[w | \bX, \bz] = e^*(\bX, \bz)$ and $\E[\by | \bX, \bz] = g^*(\bX, \bz)$.

Therefore,
\begin{equation}
P\bpsi(e^*, g^*; \bbeta^*) = \E[\bX^\top b(\bX\bbeta) - \btau(\Xi; e^*, g^*)] = \E[\bX^\top b(\bX\bbeta) - \bX^\top \by] = 0
\end{equation}
by the first-order condition~\eqref{eq:foc}. This proves that the score function is valid.

\subsection{Step 2: Neyman Orthogonality}

We show $\sqrt{n}(\mathbb{P}_n\btau(\hat{e}, \hat{g}) - \mathbb{P}_n\btau(e^*, g^*)) = o_P(1)$ by decomposing into three terms:
\begin{align}
&\sqrt{n} (\mathbb{P}_n\btau(\hat{e}, \hat{g}) - \mathbb{P}_n\btau(e^*, g^*)) \nonumber \\
&= \underbrace{\sqrt{n}\mathbb{P}_n\left[\left(1 - \frac{w}{e^*}\right) \bX^\top(\hat{g} - g^*)\right]}_{(I)} \nonumber \\
&\quad + \underbrace{\sqrt{n}\mathbb{P}_n\left[\frac{w(\hat{e} - e^*)}{\hat{e}e^*} \bX^\top(g^* - \by)\right]}_{(II)} \nonumber \\
&\quad + \underbrace{\sqrt{n}\mathbb{P}_n\left[\frac{w(\hat{e} - e^*)}{\hat{e}e^*} \bX^\top(\hat{g} - g^*)\right]}_{(III)}
\end{align}

\textbf{Term (I):} Note that $\E[(1 - w/e^*(\bX, \bz)) \bX^\top(\hat{g}(\bX, \bz) - g^*(\bX, \bz)) | I^c] = 0$ since $\E[w | \bX, \bz] = e^*(\bX, \bz)$. By the Markov inequality and Assumption~\ref{ass:ml_rate}, term (I) is $o_P(1)$.

\textbf{Term (II):} Similarly, $\E[w(\hat{e} - e^*)\bX^\top(g^* - \by)/(\hat{e}e^*) | I^c] = 0$ since $\E[g^*(\bX, \bz) - \by | \bX, \bz] = 0$. By Assumption~\ref{ass:ml_rate}, term (II) is $o_P(1)$.

\textbf{Term (III):} By Cauchy-Schwarz and Assumption~\ref{ass:ml_rate}:
\begin{equation}
\E[|\hat{e} - e^*| \|\hat{g} - g^*\|] \leq \|\hat{e} - e^*\|_{P,2} \|\hat{g} - g^*\|_{P,2} = o(n^{-1/2})
\end{equation}
Therefore term (III) is $o_P(1)$.

\subsection{Step 3: Asymptotic Expansion}

By the results above, for any sequence $\{\check{\bbeta}_n\}$ satisfying $\mathbb{P}_n\bpsi(e^*, g^*; \check{\bbeta}_n) = 0$, we have:
\begin{equation}
\sqrt{n}\mathbb{P}_n\bpsi(\hat{e}, \hat{g}; \check{\bbeta}_n) = o_P(1)
\end{equation}

\textbf{Consistency.} Define $\bar{\ell}(\Xi; e, g; \bbeta) := b(\bX\bbeta) - [g(\bX, \bz) + \frac{w}{e(\bX, \bz)}(g(\bX, \bz) - \by)]^\top \bX\bbeta$. Since $\nabla_\bbeta \bar{\ell} = \bpsi$ and $\nabla^2_\bbeta \bar{\ell} = \bX^\top \nabla^2_\theta b(\bX\bbeta) \bX \succeq 0$, the function $\bar{\ell}$ is convex. By standard arguments (cf.\ \citet{newey1994large}), $\hat{\bbeta} \to_P \bbeta^*$.

\textbf{Asymptotic Normality.} By Taylor expansion around $\bbeta^*$:
\begin{align}
\sqrt{n}(P\bpsi(\hat{\bbeta}) - P\bpsi(\bbeta^*)) &= \sqrt{n}\mathbf{J}(\hat{\bbeta} - \bbeta^*) + o_P(\|\hat{\bbeta} - \bbeta^*\|)
\end{align}

Since $\{\bpsi(\cdot; \bbeta) : \bbeta \in B(\bbeta^*, \epsilon)\}$ forms a Donsker class (by Lipschitz continuity), we have $\mathbb{G}_n\bpsi(\hat{\bbeta}) - \mathbb{G}_n\bpsi(\bbeta^*) \to_P 0$. Therefore:
\begin{equation}
\sqrt{n}(\hat{\bbeta} - \bbeta^*) = \mathbf{J}^{-1}\sqrt{n}\mathbb{P}_n\bpsi(\bbeta^*) + o_P(1)
\end{equation}

The central limit theorem yields $\sqrt{n}\mathbb{P}_n\bpsi(\bbeta^*) \rightsquigarrow N(0, \E[\bpsi\bpsi^\top])$, completing the proof. \hfill $\square$

\section{Proof of Corollary~\ref{cor:dominance}}
\label{app:proof_dominance}

When $e(\bX, \bz) = \rho$ is constant and $w \perp (\bX, \by, \bz)$, the primary-only estimator $\hat{\bbeta}^P$ solves $\sum_{i \in \mathcal{D}^P} \nabla_\bbeta \ell(\bX_i, \by_i; \bbeta) = 0$, which is equivalent to $\sum_{i=1}^n w_i \nabla_\bbeta \ell(\bX_i, \by_i; \bbeta) = 0$.

\subsection{Variance Decomposition}

Note that $\nabla_\bbeta \ell(\bX, \by; \bbeta^*) = \bX^\top(\nabla_\theta b(\bX\bbeta^*) - \by)$. Define:
\begin{align}
\zeta(\bX, \bz) &:= \bX^\top(\nabla b(\bX\bbeta^*) - g^*(\bX, \bz)) \\
\pi(\bX, \by, \bz) &:= \bX^\top(g^*(\bX, \bz) - \by)
\end{align}

Then $\nabla_\bbeta \ell = \zeta + \pi$ and $\E[\pi | \bX, \bz] = 0$.

\subsection{Primary-Only Variance}

By standard arguments:
\begin{align}
\mathbf{\Sigma}^P &= \frac{1}{\rho}\mathbf{J}^{-1}\E[(\zeta + \pi)(\zeta + \pi)^\top]\mathbf{J}^{-1} \nonumber \\
&= \frac{1}{\rho}\mathbf{J}^{-1}\E[\zeta\zeta^\top]\mathbf{J}^{-1} + \frac{1}{\rho}\mathbf{J}^{-1}\E[\pi\pi^\top]\mathbf{J}^{-1}
\end{align}
where we use $\E[\zeta\pi^\top] = \E[\E[\zeta\pi^\top | \bX, \bz]] = \E[\zeta \E[\pi^\top | \bX, \bz]] = 0$.

\subsection{DML Variance}

The DML score is $\bpsi = \zeta + \frac{w}{\rho}\pi$. Therefore:
\begin{align}
\mathbf{\Sigma}^{\text{DML}} &= \mathbf{J}^{-1}\E[(\zeta + \frac{w}{\rho}\pi)(\zeta + \frac{w}{\rho}\pi)^\top]\mathbf{J}^{-1} \nonumber \\
&= \mathbf{J}^{-1}\E[\zeta\zeta^\top]\mathbf{J}^{-1} + \frac{1}{\rho}\mathbf{J}^{-1}\E[\pi\pi^\top]\mathbf{J}^{-1}
\end{align}
where we use $\E[w] = \rho$ and independence.

\subsection{Dominance Result}

Comparing the two expressions:
\begin{equation}
\mathbf{\Sigma}^P - \mathbf{\Sigma}^{\text{DML}} = \left(\frac{1}{\rho} - 1\right)\mathbf{J}^{-1}\E[\zeta\zeta^\top]\mathbf{J}^{-1}
\end{equation}

Since $\rho < 1$, we have $\frac{1}{\rho} - 1 > 0$. The matrix $\E[\zeta\zeta^\top]$ is positive semi-definite, and strictly positive definite when $\zeta$ is not almost surely zero. This occurs when $g^*(\bX, \bz) \neq \nabla b(\bX\bbeta^*)$ with positive probability, i.e., when AI predictions are informative. \hfill $\square$

\section{Supporting Lemmas}
\label{app:lemmas}

\begin{lemma}[Score Function and Neyman Orthogonality]
\label{lem:score}
It holds that $P\bpsi(e^*, g^*; \bbeta^*) = 0$ and $\mathbb{P}_n\btau(\hat{e}, \hat{g}) - \mathbb{P}_n\btau(e^*, g^*) = o_P(n^{-1/2})$.
\end{lemma}

\begin{proof}
See Step 1 and Step 2 in Appendix~\ref{app:proof_normality}.
\end{proof}

\begin{lemma}[Rates of Empirical Scores]
\label{lem:rates}
It holds that $\|\mathbb{P}_n\bpsi(e^*, g^*; \hat{\bbeta})\|, \inf_{\bbeta \in \mathcal{B}} \|\mathbb{P}_n\bpsi(\hat{e}, \hat{g}; \bbeta)\| = o_P(1/\sqrt{n})$.
\end{lemma}

\begin{proof}
By Lemma~\ref{lem:score}, there exists a sequence $\{\check{\bbeta}_n\}$ with $\mathbb{P}_n\bpsi(e^*, g^*; \check{\bbeta}_n) = 0$ and $\check{\bbeta}_n \to_P \bbeta^*$. Then:
\begin{align}
\sqrt{n}\mathbb{P}_n\bpsi(\hat{e}, \hat{g}; \check{\bbeta}_n) &= \sqrt{n}\mathbb{P}_n\bpsi(e^*, g^*; \check{\bbeta}_n) \nonumber \\
&\quad + \sqrt{n}(\mathbb{P}_n\btau(e^*, g^*) - \mathbb{P}_n\btau(\hat{e}, \hat{g})) \nonumber \\
&= o_P(1)
\end{align}
The result follows.
\end{proof}

\begin{lemma}[Consistency]
\label{lem:consistency}
$\hat{\bbeta} \to_P \bbeta^*$.
\end{lemma}

\begin{proof}
The loss function $P\bar{\ell}(e^*, g^*; \bbeta)$ is strictly convex with unique minimizer $\bbeta^*$. By uniform convergence $\mathbb{P}_n\bar{\ell}(e^*, g^*; \bbeta) \to_P P\bar{\ell}(e^*, g^*; \bbeta)$ and Theorem 2.7 of \citet{newey1994large}, the result follows.
\end{proof}

\section{Additional Experimental Details}
\label{app:experiments}

\subsection{Data Generation}

The conjoint analysis data consists of choice tasks where each task presents two product alternatives. Each alternative is characterized by 11 attributes including price, brand, and various features. The ground truth parameters $\bbeta^* \in \R^{11}$ are known from a large-scale study with over 10,000 human responses.

\subsection{AI Prediction Generation}

We use GPT-4o to generate predictions for each choice task. The prompt describes the two alternatives and asks the model to predict which option a human respondent would choose. The model outputs:
\begin{itemize}
    \item $z = 1$: Predicts option 1
    \item $z = 0$: Predicts option 2
    \item $z = -1$: Abstains (uncertain)
\end{itemize}

The overall accuracy (excluding abstentions) is approximately 57\%.

\subsection{Implementation Details}

\textbf{G-Model.} We use stratified logistic regression with $C = 0.05$. For each value of $z \in \{-1, 0, 1\}$, we fit a separate logistic regression model on the primary data with that $z$ value.

\textbf{Cross-Fitting.} We use $K = 5$ folds. For each fold, the g-model is trained on the remaining 4 folds of primary data, then used to predict on both the held-out fold and all augmented data.

\textbf{Optimization.} The final GLM is fit using L-BFGS with the DML-adjusted targets.

\subsection{Computational Resources}

All experiments were run on a single machine with an Intel i7 processor and 32GB RAM. Each trial takes approximately 2 seconds.

\end{document}
